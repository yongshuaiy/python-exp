# verify_wikitext2.py
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.data_loader import load_wikitext2_offline


def verify_data():
    """éªŒè¯WikiText-2æ•°æ®"""
    print("=== éªŒè¯WikiText-2æ•°æ® ===")

    try:
        train_loader, val_loader, vocab = load_wikitext2_offline(
            seq_len=64,
            batch_size=4,
            use_sample_data=False  # å¼ºåˆ¶ä½¿ç”¨çœŸå®æ•°æ®
        )

        print("âœ… WikiText-2æ•°æ®åŠ è½½æˆåŠŸ!")
        print(f"è¯æ±‡è¡¨å¤§å°: {len(vocab)}")

        # æ£€æŸ¥è®­ç»ƒæ•°æ®
        train_samples = 0
        for i, (data, target) in enumerate(train_loader):
            train_samples += data.shape[0]
            if i == 0:  # åªæ£€æŸ¥ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
                print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
                print(f"ç›®æ ‡å½¢çŠ¶: {target.shape}")

                # è§£ç ç¤ºä¾‹
                sample_tokens = data[0][:15].tolist()
                sample_text = " ".join([vocab.itos.get(t, f"<unk_{t}>") for t in sample_tokens])
                print(f"æ ·æœ¬æ–‡æœ¬: {sample_text}")

                # æ£€æŸ¥æ•°æ®èŒƒå›´
                print(f"TokenèŒƒå›´: {data.min().item()} - {data.max().item()}")
                print(f"è¯æ±‡è¡¨èŒƒå›´: 0 - {len(vocab) - 1}")

        print(f"è®­ç»ƒæ‰¹æ¬¡æ•°é‡: {i + 1}")
        print(f"æ€»è®­ç»ƒæ ·æœ¬: {train_samples}")

        # æ£€æŸ¥éªŒè¯æ•°æ®
        val_samples = 0
        for i, (data, target) in enumerate(val_loader):
            val_samples += data.shape[0]

        print(f"éªŒè¯æ‰¹æ¬¡æ•°é‡: {i + 1}")
        print(f"æ€»éªŒè¯æ ·æœ¬: {val_samples}")

        return True

    except Exception as e:
        print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    success = verify_data()
    if success:
        print("\nğŸ‰ WikiText-2æ•°æ®éªŒè¯æˆåŠŸ!")
    else:
        print("\nğŸ’¥ WikiText-2æ•°æ®éªŒè¯å¤±è´¥!")